{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aerial-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorrect-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/miniconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.4 s, sys: 23.5 s, total: 51.9 s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import dataset as local pandas dataframe\n",
    "df_unique = pd.read_csv('/Users/richard/Desktop/data_cap3/interim/df_unique_tweets_hashtags_reset_index.csv',\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tough-albert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6153341 entries, 0 to 6153340\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   tweet_text  object\n",
      " 1   hashtags    object\n",
      "dtypes: object(2)\n",
      "memory usage: 140.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_unique.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-illness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6153341, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>السلام عليكم ورحمة الله وبركاته مرحبا عملاء م...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>للتأجير لبيع النطيطات زحاليق مائيه صابونية مل...</td>\n",
       "      <td>[' للتأجير', ' لبيع النطيطات', ' زحاليق مائيه ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مظلات وسواتر آفاق الرياض مظلات استراحات مظلات...</td>\n",
       "      <td>[' مظلات', ' آفاق الرياض', ' مظلات استراحات', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>فيديو شاهد مواطن يوثق بالفيديو كميات كبيرة من...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>أستغفر الله العظيم وأتوب إليه</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0   السلام عليكم ورحمة الله وبركاته مرحبا عملاء م...   \n",
       "1   للتأجير لبيع النطيطات زحاليق مائيه صابونية مل...   \n",
       "2   مظلات وسواتر آفاق الرياض مظلات استراحات مظلات...   \n",
       "3   فيديو شاهد مواطن يوثق بالفيديو كميات كبيرة من...   \n",
       "4                     أستغفر الله العظيم وأتوب إليه    \n",
       "\n",
       "                                            hashtags  \n",
       "0                                                NaN  \n",
       "1  [' للتأجير', ' لبيع النطيطات', ' زحاليق مائيه ...  \n",
       "2  [' مظلات', ' آفاق الرياض', ' مظلات استراحات', ...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-collection",
   "metadata": {},
   "source": [
    "# Trying out CAMeL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-president",
   "metadata": {},
   "source": [
    "## 2. Morphological Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exclusive-reserve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579924</th>\n",
       "      <td>يياااااااارب ســوره بالتـــوفيــــــق يــــار...</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579925</th>\n",
       "      <td>اي منهجيه ايها المرتشين المشبوهيين رد الخارجي...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579926</th>\n",
       "      <td>تم يااااااارب ســوره بالتـــوفيــــــق يــــا...</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579927</th>\n",
       "      <td>تابعو الصحفة الإنجليزية لوزارة الخارجية على ا...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579928</th>\n",
       "      <td>بالصور صحراء كبياض الثلج وجبال غريبة الشكل في...</td>\n",
       "      <td>[' العربية']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_text           hashtags\n",
       "5579924   يياااااااارب ســوره بالتـــوفيــــــق يــــار...  [' اوقاف القران']\n",
       "5579925   اي منهجيه ايها المرتشين المشبوهيين رد الخارجي...                NaN\n",
       "5579926   تم يااااااارب ســوره بالتـــوفيــــــق يــــا...  [' اوقاف القران']\n",
       "5579927   تابعو الصحفة الإنجليزية لوزارة الخارجية على ا...                NaN\n",
       "5579928   بالصور صحراء كبياض الثلج وجبال غريبة الشكل في...       [' العربية']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get subset to work with\n",
    "df_sample = df_unique.iloc[5579924:5580924].copy()\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upset-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-production",
   "metadata": {},
   "source": [
    "TO DO\n",
    "\n",
    "0. Remove repeated characters\n",
    "1. Orthographic Normalization: account for spelling inconsistencies\n",
    "2. Dediacritization\n",
    "3. Morphological Analysis\n",
    "4. Morphological Disambiguation\n",
    "5. Tokenization\n",
    "6. Dialiect Identification\n",
    "7. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-premium",
   "metadata": {},
   "source": [
    "### 0. Remove Repeating Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finite-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minimal-quarter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' يارب سـوره بالتـوفيـق يـارب اوقاف القران'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_repeating_char(df_sample.iloc[0].tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "copyrighted-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.tweet_text = df_sample.tweet_text.apply(remove_repeating_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fantastic-basis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579924</th>\n",
       "      <td>يارب سـوره بالتـوفيـق يـارب اوقاف القران</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579925</th>\n",
       "      <td>اي منهجيه ايها المرتشين المشبوهين رد الخارجيه...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579926</th>\n",
       "      <td>تم يارب سـوره بالتـوفيـق يـارب اوقاف القران</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579927</th>\n",
       "      <td>تابعو الصحفة الإنجليزية لوزارة الخارجية على ا...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579928</th>\n",
       "      <td>بالصور صحراء كبياض الثلج وجبال غريبة الشكل في...</td>\n",
       "      <td>[' العربية']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_text           hashtags\n",
       "5579924           يارب سـوره بالتـوفيـق يـارب اوقاف القران  [' اوقاف القران']\n",
       "5579925   اي منهجيه ايها المرتشين المشبوهين رد الخارجيه...                NaN\n",
       "5579926        تم يارب سـوره بالتـوفيـق يـارب اوقاف القران  [' اوقاف القران']\n",
       "5579927   تابعو الصحفة الإنجليزية لوزارة الخارجية على ا...                NaN\n",
       "5579928   بالصور صحراء كبياض الثلج وجبال غريبة الشكل في...       [' العربية']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-auditor",
   "metadata": {},
   "source": [
    "### 1. Orthographic Normalization\n",
    "Normalizing spellings to account for inconsistencies across dialects and common spelling 'mistakes'. This will reduce data sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "foreign-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extreme-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortho_normalize(text):\n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bridal-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.tweet_text = df_sample.tweet_text.apply(ortho_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sophisticated-reply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5579924             يارب سـوره بالتـوفيـق يـارب اوقاف القران\n",
       "5579925     اي منهجيه ايها المرتشين المشبوهين رد الخارجيه...\n",
       "5579926          تم يارب سـوره بالتـوفيـق يـارب اوقاف القران\n",
       "5579927     تابعو الصحفه الانجليزيه لوزاره الخارجيه علي ا...\n",
       "5579928     بالصور صحراء كبياض الثلج وجبال غريبه الشكل في...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tweet_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-cooling",
   "metadata": {},
   "source": [
    "### 2. Dediacritization\n",
    "Removing diacritics, again to reduce data sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "formed-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "western-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.tweet_text = df_sample.tweet_text.apply(dediac_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "featured-ebony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5579924                 يارب سوره بالتوفيق يارب اوقاف القران\n",
       "5579925     اي منهجيه ايها المرتشين المشبوهين رد الخارجيه...\n",
       "5579926              تم يارب سوره بالتوفيق يارب اوقاف القران\n",
       "5579927     تابعو الصحفه الانجليزيه لوزاره الخارجيه علي ا...\n",
       "5579928     بالصور صحراء كبياض الثلج وجبال غريبه الشكل في...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tweet_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-damage",
   "metadata": {},
   "source": [
    "### 3. Morphological Analysis\n",
    "Arabic has a very rich inflectional system. A verb could have up to 5400 inflections (compared to 6 in English and 1 in Chinese). So...what does a word mean? Especially when stripped of its diacritics?\n",
    "\n",
    "Perform analysis against morphological database to get all possible meanings. And then select one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "powerful-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to load a morphological database.\n",
    "# Here, we load the default database which is used for analyzing\n",
    "# Modern Standard Arabic. \n",
    "db = MorphologyDB.builtin_db()\n",
    "\n",
    "analyzer = Analyzer(db)\n",
    "\n",
    "analyses = analyzer.analyze('سيحاسب')\n",
    "\n",
    "for analysis in analyses:\n",
    "    print(analysis, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-honor",
   "metadata": {},
   "source": [
    "This only works per single word. For our purposes, better to just select the first analysis (analyses are sorted from most likely to least likely) and perform Morphological Disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-performer",
   "metadata": {},
   "source": [
    "### 4a. Simple Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "simplified-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-delay",
   "metadata": {},
   "source": [
    "First, let's split the instances of يارب and insert a whitespace in between them so that it's tokenized properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "certain-switzerland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' يارب سوره بالتوفيق يارب اوقاف القران'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ' يارب سوره بالتوفيق يارب اوقاف القران'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "military-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "yarab = 'يارب'\n",
    "ya_rab = 'يا رب'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "primary-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' يا رب سوره بالتوفيق يا رب اوقاف القران'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.replace(yarab, ya_rab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "elegant-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_yarab(text):\n",
    "    text = text.replace(yarab, ya_rab)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "romance-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.tweet_text = df_sample.tweet_text.apply(split_yarab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "concrete-angel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يا', 'رب', 'سوره', 'بالتوفيق', 'يا', 'رب', 'اوقاف', 'القران']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_word_tokenize(split_yarab(df_sample.tweet_text.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "olympic-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' يا رب سوره بالتوفيق يا رب اوقاف القران'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tweet_text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beautiful-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.tweet_text = df_sample.tweet_text.apply(simple_word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "czech-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579924</th>\n",
       "      <td>[يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579925</th>\n",
       "      <td>[اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579926</th>\n",
       "      <td>[تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579927</th>\n",
       "      <td>[تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579928</th>\n",
       "      <td>[بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...</td>\n",
       "      <td>[' العربية']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_text           hashtags\n",
       "5579924    [يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]  [' اوقاف القران']\n",
       "5579925  [اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...                NaN\n",
       "5579926  [تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...  [' اوقاف القران']\n",
       "5579927  [تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...                NaN\n",
       "5579928  [بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...       [' العربية']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-valuable",
   "metadata": {},
   "source": [
    "Something is going wrong here with the tokenization of \"ya rab\". That should tokenize to two tokens but isn't because there is no whitespace. This is causing trouble further down the line where \"yarab\" is then incorrectly disambiguated. \n",
    "\n",
    "I can resolve this by finding instances of \"yarab\" and inserting a whitespace in between. \n",
    "\n",
    "SOLVED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-winner",
   "metadata": {},
   "source": [
    "### 4. Morphological Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "capable-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "yellow-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Maximum Likelihood Disambiguator\n",
    "mle = MLEDisambiguator.pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "metric-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('نَجَحَ', 'verb', 'نَجَح-a_1')\n",
      "('بايدن', 'noun_prop', 'بايدن_0')\n",
      "('فِي', 'prep', 'فِي_1')\n",
      "('الاِنْتِخاباتِ', 'noun', 'ٱِنْتِخاب_1')\n"
     ]
    }
   ],
   "source": [
    "# The disambiguator expects pre-tokenized text\n",
    "sentence = simple_word_tokenize('نجح بايدن في الانتخابات')\n",
    "\n",
    "disambig = mle.disambiguate(sentence)\n",
    "\n",
    "# For each disambiguated word d in disambig, d.analyses is a list of analyses\n",
    "# sorted from most likely to least likely. Therefore, d.analyses[0] would\n",
    "# be the most likely analysis for a given word. Below we extract different\n",
    "# features from the top analysis of each disambiguated word into seperate lists.\n",
    "diacritized = [d.analyses[0].analysis['diac'] for d in disambig]\n",
    "pos_tags = [d.analyses[0].analysis['pos'] for d in disambig]\n",
    "lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "\n",
    "# Print the combined feature values extracted above\n",
    "for triplet in zip(diacritized, pos_tags, lemmas):\n",
    "    print(triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-world",
   "metadata": {},
   "source": [
    "The above example from the CAMeL documentation works perfectly.\n",
    "\n",
    "Let's now adapt so that we can get just the lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "touched-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(tokenized_text):\n",
    "    disambig = mle.disambiguate(tokenized_text)\n",
    "    lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "british-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['lemmas'] = df_sample.tweet_text.apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "entire-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579924</th>\n",
       "      <td>[يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "      <td>[يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, يا_1, رَبّ_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579925</th>\n",
       "      <td>[اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[أَيّ_1, مَنْهَج_1, أَيُّها_1, المرتشين_0, مَش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579926</th>\n",
       "      <td>[تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...</td>\n",
       "      <td>[' اوقاف القران']</td>\n",
       "      <td>[تَمّ-i_1, يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579927</th>\n",
       "      <td>[تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[تابِع_1, صَحْفَة_1, إِنْجلِيزِيّ_1, وِزارَة_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579928</th>\n",
       "      <td>[بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...</td>\n",
       "      <td>[' العربية']</td>\n",
       "      <td>[صُورَة_1, صَحْراء_2, بَياض_1, ثَلْج_1, جَبَل_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_text           hashtags  \\\n",
       "5579924    [يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]  [' اوقاف القران']   \n",
       "5579925  [اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...                NaN   \n",
       "5579926  [تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...  [' اوقاف القران']   \n",
       "5579927  [تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...                NaN   \n",
       "5579928  [بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...       [' العربية']   \n",
       "\n",
       "                                                    lemmas  \n",
       "5579924  [يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, يا_1, رَبّ_...  \n",
       "5579925  [أَيّ_1, مَنْهَج_1, أَيُّها_1, المرتشين_0, مَش...  \n",
       "5579926  [تَمّ-i_1, يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, ي...  \n",
       "5579927  [تابِع_1, صَحْفَة_1, إِنْجلِيزِيّ_1, وِزارَة_1...  \n",
       "5579928  [صُورَة_1, صَحْراء_2, بَياض_1, ثَلْج_1, جَبَل_...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "alert-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5579924</th>\n",
       "      <td>[يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]</td>\n",
       "      <td>[يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, يا_1, رَبّ_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579925</th>\n",
       "      <td>[اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...</td>\n",
       "      <td>[أَيّ_1, مَنْهَج_1, أَيُّها_1, المرتشين_0, مَش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579926</th>\n",
       "      <td>[تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...</td>\n",
       "      <td>[تَمّ-i_1, يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579927</th>\n",
       "      <td>[تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...</td>\n",
       "      <td>[تابِع_1, صَحْفَة_1, إِنْجلِيزِيّ_1, وِزارَة_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579928</th>\n",
       "      <td>[بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...</td>\n",
       "      <td>[صُورَة_1, صَحْراء_2, بَياض_1, ثَلْج_1, جَبَل_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet_text  \\\n",
       "5579924    [يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, القران]   \n",
       "5579925  [اي, منهجيه, ايها, المرتشين, المشبوهين, رد, ال...   \n",
       "5579926  [تم, يا, رب, سوره, بالتوفيق, يا, رب, اوقاف, ال...   \n",
       "5579927  [تابعو, الصحفه, الانجليزيه, لوزاره, الخارجيه, ...   \n",
       "5579928  [بالصور, صحراء, كبياض, الثلج, وجبال, غريبه, ال...   \n",
       "\n",
       "                                                    lemmas  \n",
       "5579924  [يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, يا_1, رَبّ_...  \n",
       "5579925  [أَيّ_1, مَنْهَج_1, أَيُّها_1, المرتشين_0, مَش...  \n",
       "5579926  [تَمّ-i_1, يا_1, رَبّ_1, سُور_1, تَوْفِيق_1, ي...  \n",
       "5579927  [تابِع_1, صَحْفَة_1, إِنْجلِيزِيّ_1, وِزارَة_1...  \n",
       "5579928  [صُورَة_1, صَحْراء_2, بَياض_1, ثَلْج_1, جَبَل_...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[['tweet_text', 'lemmas']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "satisfactory-armor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يا_1',\n",
       " 'رَبّ_1',\n",
       " 'سُور_1',\n",
       " 'تَوْفِيق_1',\n",
       " 'يا_1',\n",
       " 'رَبّ_1',\n",
       " 'وَقْف_2',\n",
       " 'قُرْآن_1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.lemmas.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "saved-assumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'سُور_1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.lemmas.iloc[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-vietnam",
   "metadata": {},
   "source": [
    "Great. We now have tokenized lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-biography",
   "metadata": {},
   "source": [
    "### 4b. Alternative Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "biblical-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-killing",
   "metadata": {},
   "source": [
    "Below, we will instantiate and define the tokenizer. There are many different schemes we can use to tokenize the text: e.g. separate tokens for prefixes and suffixes for example.\n",
    "\n",
    "Below is a list of accepted tokenization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "helpful-ending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'atbseg',\n",
       "           'atbtok',\n",
       "           'bwtok',\n",
       "           'd1seg',\n",
       "           'd1tok',\n",
       "           'd2seg',\n",
       "           'd2tok',\n",
       "           'd3seg',\n",
       "           'd3tok'})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MorphologicalTokenizer.scheme_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "senior-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يارب سوره بالتوفيق يارب اوقاف القران\n"
     ]
    }
   ],
   "source": [
    "print('يارب سوره بالتوفيق يارب اوقاف القران')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "unsigned-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يا', 'رب', 'سوره', 'بالتوفيق', 'يا', 'رب', 'اوقاف', 'القران']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tweet_text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "continuing-detail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سور_+ه', 'ب+_التوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# atbseg scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbseg')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "proper-kenya",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سور_+ه', 'ب+_التوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# atbtok scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='atbtok')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "together-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سور_+ه', 'ب+_ال+_توفيق', 'يا', 'رب', 'أوقاف', 'ال+_قرآن']\n"
     ]
    }
   ],
   "source": [
    "# bwtok scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='bwtok')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "grateful-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سوره', 'بالتوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# d1seg scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d1seg')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "hollywood-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سوره', 'بالتوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# d1tok scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d1tok')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cleared-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سوره', 'ب+_التوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# d2seg scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d2seg')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "angry-innocent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سوره', 'ب+_التوفيق', 'يا', 'رب', 'أوقاف', 'القرآن']\n"
     ]
    }
   ],
   "source": [
    "# d2tok scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d2tok')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "automatic-bridges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سور_+ه', 'ب+_ال+_توفيق', 'يا', 'رب', 'أوقاف', 'ال+_قرآن']\n"
     ]
    }
   ],
   "source": [
    "# d3seg scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3seg')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abroad-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يا', 'رب', 'سور_+ه', 'ب+_ال+_توفيق', 'يا', 'رب', 'أوقاف', 'ال+_قرآن']\n"
     ]
    }
   ],
   "source": [
    "# d3tok scheme\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok')\n",
    "tokens = tokenizer.tokenize(df_sample.tweet_text.iloc[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fossil-tours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يا_1',\n",
       " 'رَبّ_1',\n",
       " 'سُور_1',\n",
       " 'تَوْفِيق_1',\n",
       " 'يا_1',\n",
       " 'رَبّ_1',\n",
       " 'وَقْف_2',\n",
       " 'قُرْآن_1']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemmas(df_sample.tweet_text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-float",
   "metadata": {},
   "source": [
    "I think lemma's might be the preferred way to go here because we are doing Topic Modelling and the roots/lemma's give us the highest-level grouping of similar words.\n",
    "\n",
    "Alternatively, we could save df_unique with different tokenizations and do some trial/error testing to see which tokenization gives the best result. That's the advice of Dr. Habash (Director of CAMeL Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-citizen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-course",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-studio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-expression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "identified-mentor",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-hollow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from ar_wordcloud import ArabicWordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "revised-consistency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "،\n",
      "ء\n",
      "ءَ\n",
      "آ\n",
      "آب\n",
      "آذار\n",
      "آض\n",
      "آل\n",
      "آمينَ\n",
      "آناء\n",
      "آنفا\n",
      "آه\n",
      "آهاً\n",
      "آهٍ\n",
      "آهِ\n",
      "أ\n",
      "أبدا\n",
      "أبريل\n",
      "أبو\n",
      "أبٌ\n",
      "أجل\n",
      "أجمع\n",
      "أحد\n",
      "أخبر\n",
      "أخذ\n",
      "أخو\n",
      "أخٌ\n",
      "أربع\n",
      "أربعاء\n",
      "أربعة\n",
      "أربعمئة\n",
      "أربعمائة\n",
      "أرى\n",
      "أسكن\n",
      "أصبح\n",
      "أصلا\n",
      "أضحى\n",
      "أطعم\n",
      "أعطى\n",
      "أعلم\n",
      "أغسطس\n",
      "أفريل\n",
      "أفعل به\n",
      "أفٍّ\n",
      "أقبل\n",
      "أكتوبر\n",
      "أل\n",
      "ألا\n",
      "ألف\n",
      "ألفى\n",
      "أم\n",
      "أما\n",
      "أمام\n",
      "أمامك\n",
      "أمامكَ\n",
      "أمد\n",
      "أمس\n",
      "أمسى\n",
      "أمّا\n",
      "أن\n",
      "أنا\n",
      "أنبأ\n",
      "أنت\n",
      "أنتم\n",
      "أنتما\n",
      "أنتن\n",
      "أنتِ\n",
      "أنشأ\n",
      "أنه\n",
      "أنًّ\n",
      "أنّى\n",
      "أهلا\n",
      "أو\n",
      "أوت\n",
      "أوشك\n",
      "أول\n",
      "أولئك\n",
      "أولاء\n",
      "أولالك\n",
      "أوّهْ\n",
      "أى\n",
      "أي\n",
      "أيا\n",
      "أيار\n",
      "أيضا\n",
      "أيلول\n",
      "أين\n",
      "أيّ\n",
      "أيّان\n",
      "أُفٍّ\n",
      "ؤ\n",
      "إحدى\n",
      "إذ\n",
      "إذا\n",
      "إذاً\n",
      "إذما\n",
      "إذن\n",
      "إزاء\n",
      "إلى\n",
      "إلي\n",
      "إليكم\n",
      "إليكما\n",
      "إليكنّ\n",
      "إليكَ\n",
      "إلَيْكَ\n",
      "إلّا\n",
      "إمّا\n",
      "إن\n",
      "إنَّ\n",
      "إى\n",
      "إياك\n",
      "إياكم\n",
      "إياكما\n",
      "إياكن\n",
      "إيانا\n",
      "إياه\n",
      "إياها\n",
      "إياهم\n",
      "إياهما\n",
      "إياهن\n",
      "إياي\n",
      "إيهٍ\n",
      "ئ\n",
      "ا\n",
      "ا?\n",
      "ا?ى\n",
      "االا\n",
      "االتى\n",
      "ابتدأ\n",
      "ابين\n",
      "اتخذ\n",
      "اثر\n",
      "اثنا\n",
      "اثنان\n",
      "اثني\n",
      "اثنين\n",
      "اجل\n",
      "احد\n",
      "اخرى\n",
      "اخلولق\n",
      "اذا\n",
      "اربعة\n",
      "اربعون\n",
      "اربعين\n",
      "ارتدّ\n",
      "استحال\n",
      "اصبح\n",
      "اضحى\n",
      "اطار\n",
      "اعادة\n",
      "اعلنت\n",
      "اف\n",
      "اكثر\n",
      "اكد\n",
      "الآن\n",
      "الألاء\n",
      "الألى\n",
      "الا\n",
      "الاخيرة\n",
      "الان\n",
      "الاول\n",
      "الاولى\n",
      "التى\n",
      "التي\n",
      "الثاني\n",
      "الثانية\n",
      "الحالي\n",
      "الذاتي\n",
      "الذى\n",
      "الذي\n",
      "الذين\n",
      "السابق\n",
      "الف\n",
      "اللاتي\n",
      "اللتان\n",
      "اللتيا\n",
      "اللتين\n",
      "اللذان\n",
      "اللذين\n",
      "اللواتي\n",
      "الماضي\n",
      "المقبل\n",
      "الوقت\n",
      "الى\n",
      "الي\n",
      "اليه\n",
      "اليها\n",
      "اليوم\n",
      "اما\n",
      "امام\n",
      "امس\n",
      "امسى\n",
      "ان\n",
      "انبرى\n",
      "انقلب\n",
      "انه\n",
      "انها\n",
      "او\n",
      "اول\n",
      "اي\n",
      "ايار\n",
      "ايام\n",
      "ايضا\n",
      "ب\n",
      "بؤسا\n",
      "بإن\n",
      "بئس\n",
      "باء\n",
      "بات\n",
      "باسم\n",
      "بان\n",
      "بخٍ\n",
      "بد\n",
      "بدلا\n",
      "برس\n",
      "بسبب\n",
      "بسّ\n",
      "بشكل\n",
      "بضع\n",
      "بطآن\n",
      "بعد\n",
      "بعدا\n",
      "بعض\n",
      "بغتة\n",
      "بل\n",
      "بلى\n",
      "بن\n",
      "به\n",
      "بها\n",
      "بهذا\n",
      "بيد\n",
      "بين\n",
      "بَسْ\n",
      "بَلْهَ\n",
      "ة\n",
      "ت\n",
      "تاء\n",
      "تارة\n",
      "تاسع\n",
      "تانِ\n",
      "تانِك\n",
      "تبدّل\n",
      "تجاه\n",
      "تحت\n",
      "تحوّل\n",
      "تخذ\n",
      "ترك\n",
      "تسع\n",
      "تسعة\n",
      "تسعمئة\n",
      "تسعمائة\n",
      "تسعون\n",
      "تسعين\n",
      "تشرين\n",
      "تعسا\n",
      "تعلَّم\n",
      "تفعلان\n",
      "تفعلون\n",
      "تفعلين\n",
      "تكون\n",
      "تلقاء\n",
      "تلك\n",
      "تم\n",
      "تموز\n",
      "تينك\n",
      "تَيْنِ\n",
      "تِه\n",
      "تِي\n",
      "ث\n",
      "ثاء\n",
      "ثالث\n",
      "ثامن\n",
      "ثان\n",
      "ثاني\n",
      "ثلاث\n",
      "ثلاثاء\n",
      "ثلاثة\n",
      "ثلاثمئة\n",
      "ثلاثمائة\n",
      "ثلاثون\n",
      "ثلاثين\n",
      "ثم\n",
      "ثمان\n",
      "ثمانمئة\n",
      "ثمانون\n",
      "ثماني\n",
      "ثمانية\n",
      "ثمانين\n",
      "ثمنمئة\n",
      "ثمَّ\n",
      "ثمّ\n",
      "ثمّة\n",
      "ج\n",
      "جانفي\n",
      "جدا\n",
      "جعل\n",
      "جلل\n",
      "جمعة\n",
      "جميع\n",
      "جنيه\n",
      "جوان\n",
      "جويلية\n",
      "جير\n",
      "جيم\n",
      "ح\n",
      "حاء\n",
      "حادي\n",
      "حار\n",
      "حاشا\n",
      "حاليا\n",
      "حاي\n",
      "حبذا\n",
      "حبيب\n",
      "حتى\n",
      "حجا\n",
      "حدَث\n",
      "حرى\n",
      "حزيران\n",
      "حسب\n",
      "حقا\n",
      "حمدا\n",
      "حمو\n",
      "حمٌ\n",
      "حوالى\n",
      "حول\n",
      "حيث\n",
      "حيثما\n",
      "حين\n",
      "حيَّ\n",
      "حَذارِ\n",
      "خ\n",
      "خاء\n",
      "خاصة\n",
      "خال\n",
      "خامس\n",
      "خبَّر\n",
      "خلا\n",
      "خلافا\n",
      "خلال\n",
      "خلف\n",
      "خمس\n",
      "خمسة\n",
      "خمسمئة\n",
      "خمسمائة\n",
      "خمسون\n",
      "خمسين\n",
      "خميس\n",
      "د\n",
      "دال\n",
      "درهم\n",
      "درى\n",
      "دواليك\n",
      "دولار\n",
      "دون\n",
      "دونك\n",
      "ديسمبر\n",
      "دينار\n",
      "ذ\n",
      "ذا\n",
      "ذات\n",
      "ذاك\n",
      "ذال\n",
      "ذانك\n",
      "ذانِ\n",
      "ذلك\n",
      "ذهب\n",
      "ذو\n",
      "ذيت\n",
      "ذينك\n",
      "ذَيْنِ\n",
      "ذِه\n",
      "ذِي\n",
      "ر\n",
      "رأى\n",
      "راء\n",
      "رابع\n",
      "راح\n",
      "رجع\n",
      "رزق\n",
      "رويدك\n",
      "ريال\n",
      "ريث\n",
      "رُبَّ\n",
      "ز\n",
      "زاي\n",
      "زعم\n",
      "زود\n",
      "زيارة\n",
      "س\n",
      "ساء\n",
      "سابع\n",
      "سادس\n",
      "سبت\n",
      "سبتمبر\n",
      "سبحان\n",
      "سبع\n",
      "سبعة\n",
      "سبعمئة\n",
      "سبعمائة\n",
      "سبعون\n",
      "سبعين\n",
      "ست\n",
      "ستة\n",
      "ستكون\n",
      "ستمئة\n",
      "ستمائة\n",
      "ستون\n",
      "ستين\n",
      "سحقا\n",
      "سرا\n",
      "سرعان\n",
      "سقى\n",
      "سمعا\n",
      "سنة\n",
      "سنتيم\n",
      "سنوات\n",
      "سوف\n",
      "سوى\n",
      "سين\n",
      "ش\n",
      "شباط\n",
      "شبه\n",
      "شتانَ\n",
      "شخصا\n",
      "شرع\n",
      "شمال\n",
      "شيكل\n",
      "شين\n",
      "شَتَّانَ\n",
      "ص\n",
      "صاد\n",
      "صار\n",
      "صباح\n",
      "صبر\n",
      "صبرا\n",
      "صدقا\n",
      "صراحة\n",
      "صفر\n",
      "صهٍ\n",
      "صهْ\n",
      "ض\n",
      "ضاد\n",
      "ضحوة\n",
      "ضد\n",
      "ضمن\n",
      "ط\n",
      "طاء\n",
      "طاق\n",
      "طالما\n",
      "طرا\n",
      "طفق\n",
      "طَق\n",
      "ظ\n",
      "ظاء\n",
      "ظل\n",
      "ظلّ\n",
      "ظنَّ\n",
      "ع\n",
      "عاد\n",
      "عاشر\n",
      "عام\n",
      "عاما\n",
      "عامة\n",
      "عجبا\n",
      "عدا\n",
      "عدة\n",
      "عدد\n",
      "عدم\n",
      "عدَّ\n",
      "عسى\n",
      "عشر\n",
      "عشرة\n",
      "عشرون\n",
      "عشرين\n",
      "عل\n",
      "علق\n",
      "علم\n",
      "على\n",
      "علي\n",
      "عليك\n",
      "عليه\n",
      "عليها\n",
      "علًّ\n",
      "عن\n",
      "عند\n",
      "عندما\n",
      "عنه\n",
      "عنها\n",
      "عوض\n",
      "عيانا\n",
      "عين\n",
      "عَدَسْ\n",
      "غ\n",
      "غادر\n",
      "غالبا\n",
      "غدا\n",
      "غداة\n",
      "غير\n",
      "غين\n",
      "ـ\n",
      "ف\n",
      "فإن\n",
      "فاء\n",
      "فان\n",
      "فانه\n",
      "فبراير\n",
      "فرادى\n",
      "فضلا\n",
      "فقد\n",
      "فقط\n",
      "فكان\n",
      "فلان\n",
      "فلس\n",
      "فهو\n",
      "فو\n",
      "فوق\n",
      "فى\n",
      "في\n",
      "فيفري\n",
      "فيه\n",
      "فيها\n",
      "ق\n",
      "قاطبة\n",
      "قاف\n",
      "قال\n",
      "قام\n",
      "قبل\n",
      "قد\n",
      "قرش\n",
      "قطّ\n",
      "قلما\n",
      "قوة\n",
      "ك\n",
      "كأن\n",
      "كأنّ\n",
      "كأيّ\n",
      "كأيّن\n",
      "كاد\n",
      "كاف\n",
      "كان\n",
      "كانت\n",
      "كانون\n",
      "كثيرا\n",
      "كذا\n",
      "كذلك\n",
      "كرب\n",
      "كسا\n",
      "كل\n",
      "كلتا\n",
      "كلم\n",
      "كلَّا\n",
      "كلّما\n",
      "كم\n",
      "كما\n",
      "كن\n",
      "كى\n",
      "كيت\n",
      "كيف\n",
      "كيفما\n",
      "كِخ\n",
      "ل\n",
      "لأن\n",
      "لا\n",
      "لا سيما\n",
      "لات\n",
      "لازال\n",
      "لاسيما\n",
      "لام\n",
      "لايزال\n",
      "لبيك\n",
      "لدن\n",
      "لدى\n",
      "لدي\n",
      "لذلك\n",
      "لعل\n",
      "لعلَّ\n",
      "لعمر\n",
      "لقاء\n",
      "لكن\n",
      "لكنه\n",
      "لكنَّ\n",
      "للامم\n",
      "لم\n",
      "لما\n",
      "لمّا\n",
      "لن\n",
      "له\n",
      "لها\n",
      "لهذا\n",
      "لهم\n",
      "لو\n",
      "لوكالة\n",
      "لولا\n",
      "لوما\n",
      "ليت\n",
      "ليرة\n",
      "ليس\n",
      "ليسب\n",
      "م\n",
      "مئة\n",
      "مئتان\n",
      "ما\n",
      "ما أفعله\n",
      "ما انفك\n",
      "ما برح\n",
      "مائة\n",
      "ماانفك\n",
      "مابرح\n",
      "مادام\n",
      "ماذا\n",
      "مارس\n",
      "مازال\n",
      "مافتئ\n",
      "ماي\n",
      "مايزال\n",
      "مايو\n",
      "متى\n",
      "مثل\n",
      "مذ\n",
      "مرّة\n",
      "مساء\n",
      "مع\n",
      "معاذ\n",
      "معه\n",
      "معها\n",
      "مقابل\n",
      "مكانكم\n",
      "مكانكما\n",
      "مكانكنّ\n",
      "مكانَك\n",
      "مليار\n",
      "مليم\n",
      "مليون\n",
      "مما\n",
      "من\n",
      "منذ\n",
      "منه\n",
      "منها\n",
      "مه\n",
      "مهما\n",
      "ميم\n",
      "ن\n",
      "نا\n",
      "نبَّا\n",
      "نحن\n",
      "نحو\n",
      "نعم\n",
      "نفس\n",
      "نفسه\n",
      "نهاية\n",
      "نوفمبر\n",
      "نون\n",
      "نيسان\n",
      "نيف\n",
      "نَخْ\n",
      "نَّ\n",
      "ه\n",
      "هؤلاء\n",
      "ها\n",
      "هاء\n",
      "هاكَ\n",
      "هبّ\n",
      "هذا\n",
      "هذه\n",
      "هل\n",
      "هللة\n",
      "هلم\n",
      "هلّا\n",
      "هم\n",
      "هما\n",
      "همزة\n",
      "هن\n",
      "هنا\n",
      "هناك\n",
      "هنالك\n",
      "هو\n",
      "هي\n",
      "هيا\n",
      "هيهات\n",
      "هيّا\n",
      "هَؤلاء\n",
      "هَاتانِ\n",
      "هَاتَيْنِ\n",
      "هَاتِه\n",
      "هَاتِي\n",
      "هَجْ\n",
      "هَذا\n",
      "هَذانِ\n",
      "هَذَيْنِ\n",
      "هَذِه\n",
      "هَذِي\n",
      "هَيْهات\n",
      "و\n",
      "و6\n",
      "وأبو\n",
      "وأن\n",
      "وا\n",
      "واحد\n",
      "واضاف\n",
      "واضافت\n",
      "واكد\n",
      "والتي\n",
      "والذي\n",
      "وان\n",
      "واهاً\n",
      "واو\n",
      "واوضح\n",
      "وبين\n",
      "وثي\n",
      "وجد\n",
      "وراءَك\n",
      "ورد\n",
      "وعلى\n",
      "وفي\n",
      "وقال\n",
      "وقالت\n",
      "وقد\n",
      "وقف\n",
      "وكان\n",
      "وكانت\n",
      "ولا\n",
      "ولايزال\n",
      "ولكن\n",
      "ولم\n",
      "وله\n",
      "وليس\n",
      "ومع\n",
      "ومن\n",
      "وهب\n",
      "وهذا\n",
      "وهو\n",
      "وهي\n",
      "وَيْ\n",
      "وُشْكَانَ\n",
      "ى\n",
      "ي\n",
      "ياء\n",
      "يفعلان\n",
      "يفعلون\n",
      "يكون\n",
      "يلي\n",
      "يمكن\n",
      "يمين\n",
      "ين\n",
      "يناير\n",
      "يوان\n",
      "يورو\n",
      "يوليو\n",
      "يوم\n",
      "يونيو\n",
      "ّأيّان\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define stopwords\n",
    "with open('/Users/richard/Desktop/springboard_repo/capstones/three/supporting_files/arabic-stopwords.txt', 'r') as file:\n",
    "    stopwords = file.read()\n",
    "    \n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and all characters that are not arabic letters or # numbers and lemmatize the words\n",
    "# source: https://hajar-iba.medium.com/camel-tools-a-python-toolkit-for-arabic-nlp-ba9f1d2e8cb7\n",
    "def preprocess_ar(text):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    st = ISRIStemmer()\n",
    "    \n",
    "    for t in text:\n",
    "        t = ''.join(c for c in t if ud.category(c) == 'Lo' or ud.category(c) == 'Nd' or c == ' ')\n",
    "commentwords = ''\n",
    "        for word in t.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            if word not in stopwords :\n",
    "                if len(word)>1:\n",
    "                    # Lemmatizing the word.\n",
    "                    word = st.suf32(word)\n",
    "                    commentwords += (word+' ')\n",
    "    processedText.append(commentwords)\n",
    "    \n",
    "    return processedText"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
