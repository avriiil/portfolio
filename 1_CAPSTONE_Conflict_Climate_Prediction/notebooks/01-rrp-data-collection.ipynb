{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This notebook contains the code necessary to extract the relevant data from the GHCN climate .tar archive. This dataset is too large to extract in its entirety onto my machine (>50GB) and so only the relevant files are extracted. This process is computationally expensive and so is kept separate from the overall Data Wrangling which occurs in notebook 02-rrp-data-wrangling.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Import relevant packages\n",
    "2. Write function to import relevant weather data\n",
    "3. Extract files of stations within 50km of conflict incidents\n",
    "\n",
    "**IMPORTANT NOTE:** \n",
    "\n",
    "**Sections 1 and 3 can be run independently in order to allow effective running of the following notebooks (02, 03, etc.). Section 2 is superfluous to the actual running of the project and is included only as proof of progress.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to self: use kernel \"Python-geoTiledb208\" on old Macbook Pro\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "from shapely.geometry import Point, Polygon \n",
    "import pyproj\n",
    "import contextily as ctx\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', 'GeoSeries.isna', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this only when working on old MacBook Pro\n",
    "#pyproj.datadir.set_data_dir(\"/Users/richard/.conda/envs/geo/share/proj/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Functions to Extract Relevant Files From GHCN .tar Archive\n",
    "\n",
    "*This section is included only as proof of progress and is not essential to the running of the project.*\n",
    "\n",
    "**Lesson learned:** These functions were written before I was aware of the possibilities offered by spatial joins. By executing a spatial join of the climate stations on the buffered conflict data (done in notebook 02-rrp-data-wrangling) I was able to get all of the stations relevant to my analysis (i.e. those within 50km of any conflict) in one go. This proved more efficient than the functions I had written.\n",
    "\n",
    "### Before starting:\n",
    "From the documentation of the GHCN dataset we learn the following:\n",
    "\n",
    "- There are two relevant datasets for our analysis:\n",
    " 1. The GHCN_daily dataset with daily climate measures from each station\n",
    " 2. The stations dataset with metadata (name, location, etc.) of each station\n",
    " \n",
    "\n",
    "- Both of these datasets are stored in .dly format\n",
    "- The .dly format is a *fixed-width format* meaning that each column has a fixed width of characters. \n",
    "- As such we will need to specify the column widths and column labels in order to get this looking like something we can use.\n",
    "\n",
    "### Approach\n",
    "In this section, we write two functions to import the relevant GHCN data from the .tar archive:\n",
    "\n",
    "1. read_ghcn_incl_stationID: a general function that imports all .dly files from a .tar archive using the corresponding data headers and column specs.\n",
    "2. find_stations: a function that finds and extract stations with a specified radius r of a specific conflict incident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. read_ghcn_incl_stationID\n",
    "\n",
    "This is a general function that imports all .dly files from a .tar archive using the corresponding data headers and column specs. I have based my work in this section on this very useful GitLab [snippet](https://gitlab.com/snippets/1838910)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up specs for station data\n",
    "\n",
    "data_header_names = [\n",
    "    \"STATION ID\",\n",
    "    \"YEAR\",\n",
    "    \"MONTH\",\n",
    "    \"ELEMENT\"\n",
    "]\n",
    "\n",
    "data_header_col_specs = [\n",
    "    (0,  11),\n",
    "    (11, 15),\n",
    "    (15, 17),\n",
    "    (17, 21)\n",
    "]\n",
    "\n",
    "data_header_dtypes = {\n",
    "    \"ID\": str,\n",
    "    \"YEAR\": int,\n",
    "    \"MONTH\": int,\n",
    "    \"ELEMENT\": str\n",
    "}\n",
    "\n",
    "data_col_names = [[\n",
    "    \"VALUE\" + str(i + 1),\n",
    "    \"MFLAG\" + str(i + 1),\n",
    "    \"QFLAG\" + str(i + 1),\n",
    "    \"SFLAG\" + str(i + 1)]\n",
    "    for i in range(31)\n",
    "]\n",
    "\n",
    "# Join sub-lists\n",
    "data_col_names = sum(data_col_names, [])\n",
    "\n",
    "data_replacement_col_names = [[\n",
    "    (\"VALUE\", i + 1),\n",
    "    (\"MFLAG\", i + 1),\n",
    "    (\"QFLAG\", i + 1),\n",
    "    (\"SFLAG\", i + 1)]\n",
    "    for i in range(31)\n",
    "]\n",
    "\n",
    "# Join sub-lists\n",
    "data_replacement_col_names = sum(data_replacement_col_names, [])\n",
    "\n",
    "data_replacement_col_names = pd.MultiIndex.from_tuples(\n",
    "    data_replacement_col_names,\n",
    "    names=['VAR_TYPE', 'DAY'])\n",
    "\n",
    "data_col_specs = [[\n",
    "    (21 + i * 8, 26 + i * 8),\n",
    "    (26 + i * 8, 27 + i * 8),\n",
    "    (27 + i * 8, 28 + i * 8),\n",
    "    (28 + i * 8, 29 + i * 8)]\n",
    "    for i in range(31)\n",
    "]\n",
    "\n",
    "data_col_specs = sum(data_col_specs, [])\n",
    "\n",
    "data_col_dtypes = [{\n",
    "    \"VALUE\" + str(i + 1): int,\n",
    "    \"MFLAG\" + str(i + 1): str,\n",
    "    \"QFLAG\" + str(i + 1): str,\n",
    "    \"SFLAG\" + str(i + 1): str}\n",
    "    for i in range(31)\n",
    "]\n",
    "\n",
    "data_header_dtypes.update({k: v for d in data_col_dtypes for k, v in d.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ghcn_incl_stationID(filename,\n",
    "                        variables=None, include_flags=False,\n",
    "                        dropna='all'):\n",
    "    \"\"\"Reads in all data from a GHCN .dly data file\n",
    "\n",
    "    :param filename: path to file\n",
    "    :param variables: list of variables to include in output dataframe\n",
    "        e.g. ['TMAX', 'TMIN', 'PRCP']\n",
    "    :param include_flags: Whether to include data quality flags in the final output\n",
    "    :returns: Pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_fwf(\n",
    "        filename,\n",
    "        colspecs=data_header_col_specs + data_col_specs,\n",
    "        names=data_header_names + data_col_names,\n",
    "        index_col=data_header_names, #data_header_names[0]='ID'\n",
    "        dtype=data_header_dtypes\n",
    "        )\n",
    "\n",
    "    if variables is not None:\n",
    "        df = df[df.index.get_level_values('ELEMENT').isin(variables)]\n",
    "\n",
    "    df.columns = data_replacement_col_names\n",
    "\n",
    "    if not include_flags:\n",
    "        df = df.loc[:, ('VALUE', slice(None))]\n",
    "        df.columns = df.columns.droplevel('VAR_TYPE')\n",
    "\n",
    "    df = df.stack(level='DAY').unstack(level='ELEMENT')\n",
    "\n",
    "    if dropna:\n",
    "        df.replace(-9999.0, np.nan, inplace=True)\n",
    "        df.dropna(how=dropna, inplace=True)\n",
    "    \n",
    "    df.reset_index(level='STATION ID', inplace=True)\n",
    "    \n",
    "    # replace the entire index with the date.\n",
    "    # This loses the station ID index column!\n",
    "    # This will usuall fail if dropna=False, since months with <31 days\n",
    "    # still have day=31 columns\n",
    "    df.index = pd.to_datetime(\n",
    "        df.index.get_level_values('YEAR') * 10000 +\n",
    "        df.index.get_level_values('MONTH') * 100 +\n",
    "        df.index.get_level_values('DAY'),\n",
    "        format='%Y%m%d')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. find_stations\n",
    "This function finds stations within specified radius of a conflict ID and saves the corresponding .dly files as .csv in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds stations within specified radius of a conflict ID \n",
    "# and saves the corresponding .dly files as .csv in the specified directory\n",
    "\n",
    "def find_stations(conflict_id, r, directory):\n",
    "    \"\"\"Extract .dly files of GHCN weather stations within r radius of conflict incident.\n",
    "    \n",
    "    Parameters:\n",
    "    conflict_id: unique ID of conflict of interest, provided as integer. \n",
    "    r: radius within which to search for weather stations, in meters, provided as integer.\n",
    "    directory: path to existing directory where extracted .dly file will be saved as .csv, excluding the file name and extension.\n",
    "    \n",
    "    The function will save an additional .csv file in the specified directory containing only the IDs of the stations. This file will be titled radius_xx_station_IDs.csv, where xx is the radius specified in meters.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get long and lat of conflict incident as a Geometry Point\n",
    "    conflict_point = gdf_conflict_mtr.loc[conflict_id, ['geometry']]['geometry']\n",
    "\n",
    "    # 2. Create buffer using r\n",
    "    buffer = conflict_point.buffer(r)\n",
    "\n",
    "    # 3. Generate list of weather station IDs in radius\n",
    "    neighbours = gdf_stations_mtr[\"geometry\"].intersection(buffer)\n",
    "    neighbours = neighbours[~(neighbours.is_empty | neighbours.isna())] # remove null/missing values\n",
    "    \n",
    "    stations_list = []\n",
    "    for i in neighbours.index:\n",
    "        stations_list.append(i)\n",
    "    \n",
    "    df_stations = pd.DataFrame(stations_list)\n",
    "    path_stationIDs = directory + 'radius_' + str(r) + '_stationIDs.csv'\n",
    "    df_stations.to_csv(path_stationIDs)\n",
    "    \n",
    "    # 4. Extract respective .dly files to path\n",
    "    with tarfile.open(\"/Users/data_science/Desktop/datasets/GHCNdaily/ghcnd_all.tar.gz\", \"r:*\") as tar: \n",
    "\n",
    "        for ID in stations_list:\n",
    "            # extract corresponding file into dataframe\n",
    "            # add TRY-EXCEPT clause to account for missing or incorrect filenames\n",
    "            try:\n",
    "                dly_path = 'ghcnd_all/' + ID + '.dly'\n",
    "                filepath = tar.extractfile(dly_path)\n",
    "                df_ID = read_ghcn_incl_stationID(filepath)\n",
    "            except KeyError:\n",
    "                print('A KeyError was raised for station: ' + ID +'. Either the filepath is incorrect or the corresponding station file is missing.')\n",
    "\n",
    "            # turn dataframe into unique csv\n",
    "            path = directory + ID + '.csv'\n",
    "            df_ID.to_csv(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Files of Stations Within 50km of Conflict Incidents\n",
    "\n",
    "After writing the functions in section 2, I discovered the possibilities offered by performing a spatial join. This method proved more effective and efficient than the functions I wrote and so the approach is as follows:\n",
    "\n",
    "Notebook **02-rrp-data-wrangling**\n",
    "1. Creates geodataframes of the station metadata and the conflict data\n",
    "2. Executes a spatial join of the stations with the **buffered** conflict data (buffer = 50km)\n",
    "3. Extracts the unique station_IDs from that spatial join as a list\n",
    "\n",
    "Notebook **01-rrp-data-collection (i.e. THIS notebook)**\n",
    "\n",
    "4. Imports the list of unique station_IDs\n",
    "5. Extracts the .dly files of those stations\n",
    "6. Exports them as local csv files\n",
    "\n",
    "Notebook **02-rrp-data-wranging**\n",
    "\n",
    "7. Imports these csv files\n",
    "8. Combines them into df_weather\n",
    "9. Wrangles the data into workable shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE000041196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEM00041194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AF000040930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFM00040938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFM00040948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STATION ID\n",
       "0  AE000041196\n",
       "1  AEM00041194\n",
       "2  AF000040930\n",
       "3  AFM00040938\n",
       "4  AFM00040948"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import list of unique station IDs within 50km of conflicts as df\n",
    "df_stationIDs_50k = pd.read_csv('/Users/data_science/Desktop/springboard_repo/capstones/capstone-two/data/interim/stationIDs_50k.csv', index_col=0)\n",
    "df_stationIDs_50k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list\n",
    "list_stations_buf50k = []\n",
    "\n",
    "for ID in df_stationIDs_50k['STATION ID']:\n",
    "    list_stations_buf50k.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10455"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length to verify\n",
    "len(list_stations_buf50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A KeyError was raised for station: PKM00041529. Either the filepath is incorrect or the corresponding station file is missing.\n",
      "CPU times: user 20h 37min 1s, sys: 26min 33s, total: 21h 3min 35s\n",
      "Wall time: 21h 53min 57s\n"
     ]
    }
   ],
   "source": [
    "# use list to index into .tar archive and extract only the relevant station .dly files\n",
    "# export those to local .csv files\n",
    "%%time\n",
    "\n",
    "with tarfile.open(\"/Users/data_science/Desktop/datasets/GHCNdaily/ghcnd_all.tar.gz\", \"r:*\") as tar: \n",
    "\n",
    "    for ID in list_stations_buf50k:\n",
    "        # extract corresponding file into dataframe\n",
    "        # add TRY-EXCEPT clause to account for missing or incorrect filenames\n",
    "        try:\n",
    "            dly_path = 'ghcnd_all/' + ID + '.dly'\n",
    "            filepath = tar.extractfile(dly_path)\n",
    "            df_ID = read_ghcn_incl_stationID(filepath)\n",
    "        except KeyError:\n",
    "            print('A KeyError was raised for station: ' + ID +'. Either the filepath is incorrect or the corresponding station file is missing.')\n",
    "\n",
    "        # turn dataframe into unique csv\n",
    "        directory = '/Users/data_science/Desktop/springboard_repo/capstones/capstone-two/data/raw/extracted_statiÂ§\n",
    "        path = directory + ID + '.csv'\n",
    "        df_ID.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-py3.7",
   "language": "python",
   "name": "sklearn-py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
